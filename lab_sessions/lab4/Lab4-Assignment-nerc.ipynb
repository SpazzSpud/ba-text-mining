{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'words': 'EU', 'pos': 'NNP'}, {'words': 'rejects', 'pos': 'VBZ'}, {'words': 'German', 'pos': 'JJ'}, {'words': 'call', 'pos': 'NN'}, {'words': 'to', 'pos': 'TO'}, {'words': 'boycott', 'pos': 'VB'}, {'words': 'British', 'pos': 'JJ'}, {'words': 'lamb', 'pos': 'NN'}, {'words': '.', 'pos': '.'}, {'words': 'Peter', 'pos': 'NNP'}]\n",
      "203621\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "    a_dict = {\n",
    "        'words': token,\n",
    "        'pos': pos,\n",
    "        'labels': ne_label\n",
    "       # add features\n",
    "    }\n",
    "    training_features.append({'words': a_dict['words'], 'pos': a_dict['pos']})\n",
    "    training_gold_labels.append(a_dict['labels'])\n",
    "print(training_features[:10])\n",
    "training_gold_labels[:10]\n",
    "print(len(training_features))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'words': 'SOCCER', 'pos': 'NN'}, {'words': '-', 'pos': ':'}, {'words': 'JAPAN', 'pos': 'NNP'}, {'words': 'GET', 'pos': 'VB'}, {'words': 'LUCKY', 'pos': 'NNP'}, {'words': 'WIN', 'pos': 'NNP'}, {'words': ',', 'pos': ','}, {'words': 'CHINA', 'pos': 'NNP'}, {'words': 'IN', 'pos': 'IN'}, {'words': 'SURPRISE', 'pos': 'DT'}]\n",
      "['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        'words': token,\n",
    "        'pos': pos,\n",
    "        'labels': ne_label\n",
    "       # add features\n",
    "    }\n",
    "    test_features.append({'words': a_dict['words'], 'pos': a_dict['pos']})\n",
    "    test_gold_labels.append(a_dict['labels'])\n",
    "print(test_features[:10])\n",
    "print(test_gold_labels[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "\n",
      "Intances - 203621\n",
      "\n",
      "frequency of labels:\n",
      "B-ORG - 3.1\n",
      "O - 83.28\n",
      "B-MISC - 1.69\n",
      "B-PER - 3.24\n",
      "I-PER - 2.22\n",
      "B-LOC - 3.51\n",
      "I-ORG - 1.82\n",
      "I-MISC - 0.57\n",
      "I-LOC - 0.57\n",
      "\n",
      "\n",
      "Test Set:\n",
      "\n",
      "Instances - 46435\n",
      "\n",
      "frequency of labels:\n",
      "O - 82.53\n",
      "B-LOC - 3.59\n",
      "B-PER - 3.48\n",
      "I-PER - 2.49\n",
      "I-LOC - 0.55\n",
      "B-MISC - 1.51\n",
      "I-MISC - 0.47\n",
      "B-ORG - 3.58\n",
      "I-ORG - 1.8\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "print(\"Training Set:\\n\")\n",
    "print(\"Intances - %s\\n\" %(len(training_gold_labels)))\n",
    "print(\"frequency of labels:\")\n",
    "for label, count in Counter(training_gold_labels).items():\n",
    "    print('%s - %s'%(label,round((count/len(training_gold_labels))*100,2)))\n",
    "print(\"\\n\\nTest Set:\\n\")\n",
    "print(\"Instances - %s\\n\" %(len(test_gold_labels)))\n",
    "print(\"frequency of labels:\")\n",
    "for label, count in Counter(test_gold_labels).items():\n",
    "    print('%s - %s'%(label,round((count/len(test_gold_labels))*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "The training set has 203621 instances whilst the test set has 46435 which is around a 80-20 split. \n",
    "\n",
    "in terms of how often a label occurs in its respective set, both the training and test set seemed balanced overall. The label 'O' occurs 83% percent of the time in the training set and 82.5% in the test set. these close comparisons happens for all labels. \n",
    "\n",
    "The only difference between the 2 sets is the overall amount of instances as mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/tomnorton/opt/anaconda3/lib/python3.9/site-packages (0.0.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "cutoff = len(training_features)\n",
    "the_array = training_features\n",
    "the_array.extend(test_features)\n",
    "the_array = vec.fit_transform(the_array)\n",
    "training = the_array[:cutoff]\n",
    "testing = the_array[cutoff:]\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.81      0.78      0.79      1668\n",
      "      B-MISC       0.78      0.66      0.72       702\n",
      "       B-ORG       0.79      0.52      0.63      1661\n",
      "       B-PER       0.86      0.44      0.58      1617\n",
      "       I-LOC       0.62      0.53      0.57       257\n",
      "      I-MISC       0.57      0.59      0.58       216\n",
      "       I-ORG       0.70      0.47      0.56       835\n",
      "       I-PER       0.33      0.87      0.48      1156\n",
      "           O       0.98      0.98      0.98     38323\n",
      "\n",
      "    accuracy                           0.92     46435\n",
      "   macro avg       0.72      0.65      0.65     46435\n",
      "weighted avg       0.94      0.92      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### [ YOUR CODE SHOULD GO HERE ]\n",
    "from sklearn.metrics import classification_report\n",
    "lin_clf.fit(training, training_gold_labels)\n",
    "predict = lin_clf.predict(testing)\n",
    "print(classification_report(test_gold_labels, predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/tomnorton/opt/anaconda3/lib/python3.9/site-packages (4.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /Users/tomnorton/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/tomnorton/opt/anaconda3/lib/python3.9/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/tomnorton/opt/anaconda3/lib/python3.9/site-packages (from gensim) (1.20.0)\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# your code here\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\n\u001b[1;32m      3\u001b[0m word_embedding_model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mKeyedVectors\u001b[39m.\u001b[39mload_word2vec_format(\u001b[39m'\u001b[39m\u001b[39mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[39m'\u001b[39m, binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import gensim\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_words = []\n",
    "#for token, pos, ne_label in train.iob_words():\n",
    "    #a_dict = {\n",
    "        #'words': token\n",
    "    #}\n",
    "    #training_words.append(a_dict['words'])\n",
    "\n",
    "#test_words = []\n",
    "#for token, pos, ne_label in test.iob_words():\n",
    "    #a_dict = {\n",
    "        #'words': token\n",
    "       # add features\n",
    "    #}\n",
    "    #test_words.append(a_dict['words'])\n",
    "    \n",
    "#print(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#both_input=[]\n",
    "#for token in (training_words + test_words): #the next word from the tokenized text\n",
    "    # we check if our word \n",
    "    # is inside the model vocabulary (loaded with the Google word2vec embeddings)\n",
    "    #if token in word_embedding_model:\n",
    "        # in this case the word was found and vector is assigned with its embedding vector as the value\n",
    "        #vector=word_embedding_model[token]\n",
    "    #else: \n",
    "        # if the word does not exist in the embeddings vocabulary, \n",
    "        # we create a vector with all zeros.\n",
    "        # The Google word2vec model has 300 dimensions so we creat a vector with 300 zeros\n",
    "        #vector=[0]*300\n",
    "        #print('This word is not in the word2vec vocabulary:', token)\n",
    "    #both_input.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/42krrmz573z3bz3qw2zt33lw0000gn/T/ipykernel_51148/3935160730.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  kaggle_dataset = pandas.read_csv(path, error_bad_lines=False)\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x85 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1233\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1246\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1444\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x85 in position 0: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m##### Adapt the path to point to your local copy of NERC_datasets\u001b[39;00m\n\u001b[1;32m      2\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mner_dataset.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m kaggle_dataset \u001b[39m=\u001b[39m pandas\u001b[39m.\u001b[39mread_csv(path, error_bad_lines\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(inspect\u001b[39m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1772\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1765\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1766\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1767\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m     (\n\u001b[1;32m   1769\u001b[0m         index,\n\u001b[1;32m   1770\u001b[0m         columns,\n\u001b[1;32m   1771\u001b[0m         col_dict,\n\u001b[0;32m-> 1772\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1773\u001b[0m         nrows\n\u001b[1;32m   1774\u001b[0m     )\n\u001b[1;32m   1775\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1776\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/c_parser_wrapper.py:243\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 243\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    244\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1090\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1233\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1246\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._string_convert\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/_libs/parsers.pyx:1444\u001b[0m, in \u001b[0;36mpandas._libs.parsers._string_box_utf8\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x85 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = 'ner_dataset.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
